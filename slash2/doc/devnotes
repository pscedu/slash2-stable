Alternative CREAT strategy
- requires that the client has a pre-assigned portion of the inode # space
- need some sort of directory locking to handle O_EXCL
- write enabled lookup intent of the directory would be very useful

MDS fcoo is open or closed upon arrival of first bmap or release of last bmap.

Most stateful operations now hinge on SRMT_GETBMAP
	•	modification of replication table
	•	creation of slash2 inode data
	•	open of zfs inode and storage of its fd object
	•	logging of bmap assignment and client lease (for now each is done as a separate operation)

	On Create():
		Client must have authenticated the create
		New inode is journaled with slashID, parent ino slashID, and name
			. note that the return code from the ensuing create is what will be returned to the
			client.  O_EXCL provides a good example, 2 creates for the same file may
			have arrived from 2 different clients.
			. garbage collection must be taken into account here.  Which means that creates					must be journaled with their appropriate flags (O_CREAT, O_TRUNC, etc.).
			. when the journal shadow cache is processed, create operations can be analyzed
			to determine whether GC operations are needed.  Should the sl_gen of the object
			be > 0 then slfid_(slgen-1) must be GC'd.  But how do we deal with replicas				information without upcall'ing from the zfs layer?
			. what about synchronous truncates?

		Call zfsslash2_opencreate(), make new zfs inode (no slash2 inode yet)
			. is there any way to delay this?
			. yes.  The client will have a cached copy but it may be unwise to pursue this				route since there's no guarantee that the server will create the inode in a timely				fashion.
		Close the zfs fd.
		Generate fdbuf which is sent back in the reply
			. fdbufs are needed to guarantee that io requests between the client and mds are
			not manipulated.

		What about (O_CREAT | O_TRUNC)?  Need the ability to bump the secondary gen or
			'sgen' number.  The 'sgen' lives in the zfs inode

	On Open():
		client authenticates the open.
		Mds returns fdbuf

MDS FCMH Cache Modifications
To prevent excessive 'by-id' lookups in the mds namespace, it will be
necessary for us to maintain fcmh's on the server which can translate
between slash inode numbers and their local zfs counterparts.  The
current version of the client fidcache caches the zfs inode which is
later presented to the mds, whose zfs layer has the inode cached.  Since
we'll be moving from zfs inode numbers to self-managed inodes in slash2
the mds-side fcmh cache will have to be provisioned differently.

'Clean' fcmh denomination should be modified to support fcmh's with open
fd's.  This will be useful on sliod and slashd.


Bmap Release Code (Paul)
	•	bmap client leases must be logged to disk via odtable (bdbuf stored there??) to maintain state across slashd reboots
	◦	sliod must release local file descriptor when fcoo / bmap tree is empty
	•	the client times out his own bmaps, messaging the mds who in turn contacts sliod if the bmap refcnt is 0.
	◦	This will double as a lazy coherence measure should the client miss or ignore a bmap callback from the mds.  Unless the timeout is conditional upon the bmap's idle time, which in that case would allow us to quickly remove idle bmaps.  Bmaps assigned to open files may be held longer than those belonging to closed files – who may be reaped immediately.
	◦	In this scenario, how should the client details be managed?
	▪	Free local bmap first, RPC second?
	▪	What happens on client disconnect?  What if the client disconnect from the MDS but not the ION?  Is there a way we can revoke specific bdbuf tokens?  For instance, a client with whom we've lost contact has his bmaps revoked after 60 seconds by the mds.  The mds informs the sliod of the bdbufs which have been revoked.  Another strategy is where the mds places a timestamp in the bdbuf, this is secure but relies on some degree of clock synchronization.  The previous method requires the bmap to track all open bmap handles.
	▪	What if the sliod tracked the timeouts of all valid bdbufs with which he interacts? This allows him to fail I/O's for bdbuf's which have timed out.  We also need some sort of counter which the sliod obtains on connect (to the mds) which informs him of valid bdbufs outstanding in the network.  For these bdbufs he may be required to handle I/O's.  In fact, the mds could send back the remaining time on all outstanding bmaps.  One issue is that, currently, the client cannot release a bmap which has outstanding operations.
	▪	sbs_bmap_cntr could be used a global counter, assigned by the mds, which signifies the oldest bdbuf in the network.  Since bdbufs have a static maximum timeout such an approach could work.
	•		By definition:  bmap_cntr(t0) is older than bmap_cntr(t1)
	•	So on startup, sliod connects to the mds and get the last valid bmap_cntr.  The bmap_cntr could (or could not) be aware of the given sliod's context (ie what bmaps have been assigned to it).  Additionally, updates of this value could be piggybacked on all RPC replies from the mds.  So the sliod can do his own timeout work with help from the mds.   The release msgs from the mds should note the oldest allowable bmap and any explicitly closed bmap_cntr's which have seen an early close.
	▪	What happens on the client side when his bmap times out?  Note that I/O's may be pending.  The sliod should give a small timeout leeway so that msgs don't timeout inflight, say 1 – 5 seconds?
	◦	Bmaps which are timing out need to marked by the release thread so that we don't run into problems.
	◦	_bmap_get() should be modified to retry if the bmap in question is being timed out.


	◦	An alternate plan could involve sliod being able to timeout his own bmaps, notifying slashd that he would like to do so.  This method could be sloppy because the client isn't aware that his bdbuf's have been invalidated.

File size Handling for Open Files (Paul)
	The mds recv's asynchronous updates from the sliod which update
	the s2size in zfs.  While this method prevents stat()'s of the
	file object on the ION, the file size is susceptible to
	'shrinking' should the cached size attributes time out prior to
	the mds receiving a size update from the ION.  In certain
	instances the file size must come from the ION which is handling
	the file.  This is only for clients who wish to do IO on the
	file.  For instance, open(.., O_APPEND) requires an accurate
	file size.

Can logical clocks be used here?  It seems that the sliod could maintain
a logical clock which would be passed between the client and mds.  The
value could be sent to the client by piggybacked with the reply of a
write rpc.  The current design already sends size data to slashd via the
crc update path.  The client could use the clock number to determine
whose values were more current, his or those reported by slashd.  Issues
surrounding the failure of an mds or sliod remain.  Sliod counters would
be reset upon reboot, while the mds could track these counters in logs
and/or odtable.

On sliod reboot, the mds is going to connect to him and ask for the
status of his leased bmaps.  At that time the mds could provide him the
last known sequence number.   The problem is that the client may be
holding a higher number which neither the freshly booted sliod or mds
know of once slashd recv's updated info from newly booted sliod for a
given bmap.   We need a way to inform clients that their sequence
numbers are irrelevant and that the mds has the most recent version of
the attributes.

02/12/10
Jared suggests using a partial truncate sequence number and truncate
operations administered between the client and sliod.  Sliod then
informs slashd via crc update rpc's or some similar mechanism.  Are
there issues when the sliod doesn't have the full set of bmaps resident
on his local storage.  What happens when bmaps past the truncation point
are in use by other clients and/or sliods?

For the moment we will pursue Jared's strategy which uses a persistent
counter which is increased upon each partial truncate operation
(PTRUNC).  With this strategy, the client always assumes the highest
file size unless he encounters a PTRUNC value which is greater than the
value he holds in memory.

Some important realizations regarding design of O_APPEND were made.
Write RPC's to sliod on behalf of O_APPEND fd's should be handled
differently than standard writes which being at offset 0.  O_APPEND
writes do not require the client to send the offset to the sliod.
Instead, sliod applies the write to the current EOF for that object.
This model relieves us from requiring strict file size correctness
between clients using O_APPEND.  Additionally we noted that file size
and PTRUNC value should be piggybacked onto the write RPC reply so that
the client can learn the new file size.

Garbage Collection (Paul)

Notes on truncate()
	. fuse implements truncate via setattr which means the file must
	be open for write			VOP_SPACE()
	. truncates via open(O_CREAT) are done with VOP_CREATE()

Asynchronous truncates vs. Synchronous
1) Asynchronous will rely on generation numbering to determine the freshness of objects.  Objects opened with (O_CREAT | O_TRUNC) or otherwise truncated to '0' would be granted a new generation number, which in turn, would be used to create a new object container on the IO node.
	Steps
	MDS
	1.	Recv truncate operation from client
	2.	Log create request in the slash journal:  parent sl_ino, fname, child sl_ino, create flags (create flags let us know if there's a truncate) Note that the sl_ino may be used as an  operation sequence number  if there's a way to prevent them from pre-allocated sl_ino's from leaking from failed create operations. (Jared's windowing code may be a good fit here).
	3.	Execute the create request within zfs.  On failure, restore the pre-allocated sl_ino to the window / pool but don't journal any additional information.
	1.	Within zfs we must establish a callback to be executed when O_TRUNC has been requested on a preexisting file.
	2.	In this situation the callback will log the sl_inode's replication table into the journal along with the sl_inode and sl_gen.  The new sl_gen will be logged by inference.
	3.	The sl_gen number will be  applied within the zfs inode.
	4.
	1.

2) Synchronous truncates would require the requesting client to perform the operation.  What does this entail?
		 - Notifying clients of the affected bmaps (ie bmap # and >) of the new ION handling the truncate and possibly future write operations.  Forthcoming writes into invalidated replicas must not occur until truncates have happened on the invalidated objects.  Failure to do so could cause situations where data 'holes' are not treated as such.
		- How should the truncate of the invalidated replicas be handled?
			. so long as the mds has a mechanism to prevent the lease of un-truncated bmaps
			this should not be a problem.  Using a bit in the bh_repls array can provide the
	1.				necessary state.
Steps:
	1.	MDS
	1.	Recv truncate operation from client
	2.	Locate affected bmap(s)
	1.	The bmap containing the truncation point is assigned to an ION
	2.	Other bmaps within or past the truncation point, which are resident on other IOS's are marked as SL_REPLST_TRUNCPNDG.  As with other bh_repl modifications, these are modifications are journaled.
	3.	Clients accessing SL_REPLST_TRUNCPNDG bmaps are notified to drop their leases.
	4.	SL_REPLST_TRUNCPNDG bmaps are enqueued for truncation by their respective  IOS's.   This step is not performed inline with the operation but rather asynchronously when the mds journal shadow is processed.
Client
	1.	Recv bmap lease from MDS
	2.	Send truncate RPC to prescribed ION
	1.	On success, return to the application
	2.	Otherwise fail the truncate operation.

	•	should the client or the MDS perform the synchronous
	truncate rpc?  Should the mds perform the operation he would be
	aware of the return code.  Otherwise the client will somehow
	need to communicate to the mds that the operation failed.  The
	protocol above assumes success but has the advantage that the
	client and IOS are more likely to be in close proximity to one
	another.
	•	As of 02/03/10, I'm leaning toward having the mds negotiate the operation.
	1.


Development Laundry List
	1.	Journal Shadowing
	1.	Processing or distilling of shadow entries for mds replication logging and garbage collection.
	2.	FCMH Cache Changes (see MDS FCMH Cache Modifications)
	1.	fcmh must handle slash inode changes
	2.	slashd and sliod fcmh reaping needs to work with fcmh's who have open fcoo's but reside on the clean list.
	3.	ZFS Callback table
	1.	Model after stream id's in zest
	1.	Closeable once all fuse references are gone and all bmaps are gone
	3.	Maintain fdbuf's
	4.	Fcoo doesn't close until all mfh's are released
	5.
	1.	The fcoo is closeable once all bmaps have been released
	2.	posix and zfs file descriptors are closed on destroy
	3.	This change may require modifications to the fcoo structure
	1.	The fdbuf is really only needed on the client.  Should it be statically allocated in the fcoo?
	7.	Slashd:  removal of mexpfcm's and cfd's, just track client bmap leases.

